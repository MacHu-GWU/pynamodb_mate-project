{
 "cells": [
  {
   "cell_type": "raw",
   "id": "32b4c79be198afde",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ".. _use-dynamodb-as-crawler-metadata-store-backend:\n",
    "\n",
    "Use DynamoDB as Crawler Metadata Store Backend\n",
    "======================================================================"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5e85b4139e30e9e",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This :ref:`store-large-object-in-dynamodb` document demonstrates the pattern of storing large data on S3 and storing the S3 URI as a DynamoDB item attribute. The :ref:`status-tracking` document shows the usage of ``pynamodb_mate`` to track the status of business-critical tasks. In this document, we will show you how to integrate these two patterns together for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039838eee0a5d08",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A Real Work Example: Web Crawling\n",
    "\n",
    "Let's say you have a lot of URLs from which you want to extract data from their HTML. The best practice is to retrieve the valid HTML data and store it on S3, then take your time to improve your data parser. This way, you don't have to re-crawl the HTML data if your parser has a bug and needs improvement.\n",
    "\n",
    "With this solution, you can put all the URLs you want to crawl into DynamoDB as tasks. Then, use the status tracking mechanism to query the \"unfinished\" tasks and start crawling. You can store the large HTML data on S3 and update the corresponding task in DynamoDB accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355047164a910140",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Import pynamodb_mate and Some Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1619d261e8af9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import typing as T\n",
    "import base64\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import requests\n",
    "from s3pathlib import S3Path, context\n",
    "from boto_session_manager import BotoSesManager\n",
    "import pynamodb_mate.api as pm\n",
    "from rich import print as rprint\n",
    "\n",
    "st = pm.patterns.status_tracker\n",
    "la = pm.patterns.large_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f6a09be14c522",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Setup AWS Credential and Some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3f6e5b081a762",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# use default profile\n",
    "bsm = BotoSesManager()\n",
    "# let the S3pathlib library to know which boto session to use\n",
    "context.attach_boto_session(bsm.boto_ses)\n",
    "# let the pynamodb_mate library to know which boto session to use\n",
    "conn = pm.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499d9c08ea68aab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_utc_now() -> datetime:\n",
    "    return datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "\n",
    "\n",
    "# We use b64encoded string of the url as the task_id.\n",
    "def b64encode_url(url: str) -> str:\n",
    "    return base64.urlsafe_b64encode(url.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def b64decode_url(b64: str) -> str:\n",
    "    return base64.urlsafe_b64decode(b64.encode(\"utf-8\")).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525041bbff2dd39a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Declare DynamoDB ORM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed8cfed354c5fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This code follows the pattern in the \n",
    "# \"Enable status tracking for business critical application using Amazon DynamoDB\"\n",
    "# document\n",
    "class StatusEnum(st.BaseStatusEnum):\n",
    "    pending = 10\n",
    "    in_progress = 20\n",
    "    failed = 30\n",
    "    succeeded = 40\n",
    "    ignored = 50\n",
    "\n",
    "\n",
    "class Task(\n",
    "    st.BaseTask,\n",
    "    # put large attribute mixin class here\n",
    "    pm.patterns.large_attribute.LargeAttributeMixin,\n",
    "):\n",
    "    class Meta:\n",
    "        table_name = \"pynamodb-mate-test-crawler-example\"\n",
    "        region = bsm.aws_region\n",
    "        billing_mode = pm.constants.PAY_PER_REQUEST_BILLING_MODE\n",
    "\n",
    "    html: pm.OPTIONAL_STR = pm.UnicodeAttribute(null=True)\n",
    "\n",
    "    status_and_update_time_index = st.StatusAndUpdateTimeIndex()\n",
    "\n",
    "    config = st.TrackerConfig.make(\n",
    "        use_case_id=\"crawler\",\n",
    "        pending_status=StatusEnum.pending.value,\n",
    "        in_progress_status=StatusEnum.in_progress.value,\n",
    "        failed_status=StatusEnum.failed.value,\n",
    "        succeeded_status=StatusEnum.succeeded.value,\n",
    "        ignored_status=StatusEnum.ignored.value,\n",
    "        n_pending_shard=1,\n",
    "        n_in_progress_shard=1,\n",
    "        n_failed_shard=1,\n",
    "        n_succeeded_shard=1,\n",
    "        n_ignored_shard=1,\n",
    "        status_zero_pad=3,\n",
    "        status_shard_zero_pad=3,\n",
    "        max_retry=3,\n",
    "        lock_expire_seconds=15, # an HTTP request should be done in 1-5 seconds.\n",
    "    )\n",
    "\n",
    "    # the partition key attribute name is always \"key\"\n",
    "    # which is defined by status tracking module\n",
    "    # in this use case, the task_id is the b64encoded url\n",
    "    # so that it fits into the S3 path\n",
    "    @property\n",
    "    def url(self):\n",
    "        return b64decode_url(self.task_id)\n",
    "\n",
    "# Create table\n",
    "Task.create_table(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac022b5162ca6f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create S3 bucket\n",
    "# You could overwrite the bucket and s3path\n",
    "bucket = f\"{bsm.aws_account_alias}-{bsm.aws_region}-data\"\n",
    "s3dir_root = S3Path(f\"s3://{bucket}/projects/pynamodb_mate/examples/use-dynamodb-as-crawler-metadata/{Task.config.use_case_id}/html/\").to_dir()\n",
    "# You could manually create the S3 bucket\n",
    "bsm.s3_client.create_bucket(Bucket=s3dir_root.bucket);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a9e083e026739",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# delete everything in DynamoDB and S3 to ensure a fresh start\n",
    "# PLEASE carefully review the bucket path before doing so!\n",
    "# Task.delete_all()\n",
    "# s3dir_root.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0bd04cff6ce6d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Initialize a Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82775bbbc9155576",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the url to crawl as a task\n",
    "url = \"https://www.python.org/\"\n",
    "task_id = b64encode_url(url)\n",
    "task = Task.make_and_save(task_id=task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302effb2249d7dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Define Your Task Logic\n",
    "\n",
    "In this use case, the task logic is all about get the HTML of the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281315d97affe63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CrawlError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def do_one_task(task_id: str):\n",
    "    exec_ctx: st.ExecutionContext\n",
    "    with Task.start(task_id=task_id, detailed_error=True, debug=True) as exec_ctx:\n",
    "        task: Task = exec_ctx.task\n",
    "        res = requests.get(task.url)\n",
    "        # if status code is not 200, we consider the task as \"failed\"\n",
    "        if res.status_code != 200:\n",
    "            raise CrawlError(f\"Failed to download {url}\")\n",
    "        html = res.text\n",
    "        utc_now = get_utc_now()\n",
    "        Task.update_large_attribute_item(\n",
    "            s3_client=bsm.s3_client,\n",
    "            pk=task.key,\n",
    "            sk=None,\n",
    "            # large attribute only stores binary\n",
    "            # if the user data is not binary, serialize it to binary\n",
    "            kvs=dict(html=html.encode(\"utf-8\")),\n",
    "            bucket=s3dir_root.bucket,\n",
    "            prefix=s3dir_root.key,\n",
    "            update_at=utc_now,\n",
    "            # you can pass additional arguments to the underlying\n",
    "            # s3_client.put_object API call\n",
    "            # in this example, this content type allow you to open\n",
    "            # the HTML in web browser without downloading the S3 object\n",
    "            s3_put_object_kwargs=dict(\n",
    "                html={\n",
    "                    \"ContentType\": \"text/html\",\n",
    "                },\n",
    "            ),\n",
    "            clean_up_when_succeeded=True,\n",
    "            clean_up_when_failed=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eaeb3299850fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Crawl Unfinished Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244dd934e5459ea3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use query_for_unfinished API to get unfinished data\n",
    "task_list = Task.query_for_unfinished(limit=10).all()\n",
    "rprint(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f1b077ace8f86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for task in task_list:\n",
    "    do_one_task(task.task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6d41ef6af4f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Verify The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a75c2a58cfcf89",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# You can see the status got updated to \"succeeded\"\n",
    "# And the html attributes is the S3 uri\n",
    "task = Task.get_one_or_none(task_id)\n",
    "rprint(task.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f02aa35c9d4dc9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# You can read the HTML data from S3\n",
    "print(S3Path(task.html).read_text()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab30ada56bec43f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# You can also preview the HTML in S3 console without downloading it\n",
    "print(f\"preview crawled HTML at: {S3Path(task.html).console_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0ddb7508e6e8c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The `pynamodb_mate` library is designed to be highly modular. You can easily combine and integrate its features according to your specific requirements, providing flexibility and ease of use in building your application.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
